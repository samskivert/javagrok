\section{Evaluation}
\label{sec:Evaluation}

We plan to perform a small experiment wherein we will measure events where a
developer encounters a question of how to correctly use a third party library.
We consider three cases: a) the question was answered by the library
documentation, b) the question was not answered by the library documentation,
but was answered by direct referal to the library source code, and c) the
question was answered neither by the documentation nor the source code.

Our hypothesis, construed in terms of this experiment, is that by augmenting
library documentation with the results of the previously described analyses, we
will increase the incidence of questions being answered by the library
documentation and reduce the incidence of questions requiring perusal of the
source code. We do not hypothesize that our augmented documentation will have
any impact on the incidence of questions that are answered by neither the
documentation nor source.

\subsection{Experiment Design}
The experiment will involve two groups of developers, each given a library with
which they are unfamiliar and a set of programming tasks to be implemented
using the functionality provided by the library. One group will be provided
with augmented documentation and the other will be provided with original,
unmodified documentation.

The members of each group will be provided with equivalent development
environments (the Eclipse IDE, configured to provide ready access to the
augmented or unmodified documentation as well as the augmented or unmodified library source code).
Additionally the subjects will be shown how to access both the library
documentation and source code to reduce the likelihood that differing
familiarity with the development environment influences the subject's decision
to inspect the documentation or source code.

The subjects will be instructed to first check the documentation when they
encounter a question about correct usage of the library, then to consult the
source code only if their question was not answered to their satisfaction by
the documentation. Finally they will record the outcome of each event as one
of: ``resolved by documentation'', ``resolved by source code'' or
``unresolved''.

We will also instruct subjects to complete a short survey after they have
completed the experimental tasks. The results of this survey are not intended
to directly validate or refute our hypothesis, but to help us to gauge other
aspects of the work, better understand ambiguous results, and to direct future
work. The survey questions will include:

\begin{itemize}
\item Do you normally use an integrated development environment (IDE)?
\item Do you normally rely more on documentation or source code when
  understanding new code?
\item Did you find the experience of using this library to be pleasant, normal,
  or frustrating?
\item Did you find the library documentation to generally be to too detailed,
  sufficiently detailed, or not detailed enough?
\item Was anything documented that you found unnecessary?
\item Was anything not documented that you wished had been present?
\end{itemize}

\subsection{Experiment Dry Run}

To identify problems with our experiment design and gauge the level of
difficulty of our programming tasks, we conducted a dry run of our experiment,
where the authors (minus the one who designed the experiment tasks) acted as
non-control-group subjects. 
The programming tasks revolved around using an animation library, Nenya, to put
together several simple animations, with some guidance for each task as to which
parts of the library would probably be useful.
The results were humbling and informative.

Small omissions in the instructions for the tasks resulted in substantial
wasted time on the part of the subjects. Further, most questions encountered by
the subjects were answered neither by the documentation nor the
source code. This was partly due to the lack of results from two of the
inference algorithms (nullability and mutability) which were not available due
to technical difficulties at the time of the experiement. At least one of the
questions encountered by one of the subjects would likely have been addressed
by nullability information.  A more common reason for unanswered questions was
unfamiliarity with the kind of task at hand, leading to more questions about
higher-level intended usage patterns for the library, rather than questions about the
properties we infer.

We also had not provided a standard Eclipse environment in our dry run, and
discovered that developers using the command line with simpler editors may be biased against
checking the source code for information on library usage because of difficulty
in locating the necessary source file among the experiment files.

In addition to the standardization of development environment, we plan to
reconsider our experiment tasks and potentially our target library. We will aim
for tasks that avoid implict domain knowledge (in this case, graphics and UI
library usage patterns). Though this higher level usage pattern information is
also commonly lacking in library documentation, we do not hypothesize that
analysis augmented documentation will address such shortcomings.
