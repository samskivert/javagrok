\section{Introduction}

Despite their best intentions, many developers routinely fail to provide
adequate or any documentation of the code they write.  Whether because of bad
practice, laziness or a sincere belief on the part of programmers that their
code will soon be thrown away, much code in regular use remains undocumented.
New developers join the team, the code gets handed off to another group, or
perhaps even posted publicly on the Internet.  By various means, this code
finds its way into the hands of programmers who---having been assured that this
code will save them weeks of effort---now face a thoroughly unenviable task:
grok a lump of un(der)documented code and figure out its interface well enough
to solve their original problem.

Ideally, we would have an army of ingenious, classically trained Shakespearean
typing monkeys, ready to provide high-quality documentation for all our
programs and libraries at the drop of a hat.  However in reality, we, the
unlucky developers often must make do with a single use case or oblique e-mail
offering advice---if we get anything at all.  Sitting at our desks, we curse
fate, the code in front of us, and the anonymous (or not so anonymous)
programmer responsible for our pain.  We wonder, wouldn't it be nice if we
could just push a button and get some useful, if not perfect documentation?

We propose that such documentation can be provided automatically via the
application of existing static analysis techniques to the code along with
translation of their results into human readable form and integration with any
documentation that may have been provided for the library. We specifically
consider nullability, mutability, leaking and capturing and exceptional
condition analyses, though others may also yield useful documentation. We will
evaluate the effectiveness of these analyses by comparing the documentation
they generate to known good documentation, and by conducting a user study where
two groups of developers perform programming tasks involving a library, one
with analysis augmented documentation and one without.

We will also developed a tool, JavaGrok, to automate the process of applying a
suite of analyses to a library code base and integrating their results into the
Javadoc documentation.
