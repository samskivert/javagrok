\section{Conclusion}

The results of our user evaluation were inconclusive.  We did not reach any reliable conclusion on whether automatically generated documentation annotations are helpful to programmers in practice.  There are a number of possible reasons our study was ineffective.

First and most significantly, we found it very difficult to pick a suitable task.  We rejected many tasks because they were too simple.  These tasks were not frustrating and appeared unlikely benefit from any form of documentation.  On the other hand, the chosen tasks were frustrating, but primarily due to a lack of informal documentation and use examples.  The formal properties that our analyses inferred proved to be mostly irrelevant.

None the less, many test subjects thought our annotations would be useful.  This leads us to believe that the principal failing of our experimental design was duration of task more than any other factor.  We expect that a longer in-situ industrial study would be more likely to provide meaningful conclusions (positive or negative) about the utility of automatically generated documentation.

When we began this project, we chose to find and use pre-existing analyses in order to save time.  In hindsight, this may have been the wrong choice.  The analyses were surprisingly difficult to get working and integrated together.  Once the analyses were integrated and run, we found that the results were somewhat ill-suited to our documentation purposes. (e.g. the lack of \@Nullable annotations noted by one of our test subjects) We now believe that creating special purpose analyses is a more promising and effective approach to automatically annotating documentation.

