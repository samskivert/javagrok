\section{Conclusion}

The results of our user evaluation were inconclusive.  We did not reach any reliable conclusion on whether javagrok annotations are helpful to programmers in practice.  There are a number of possible reasons our study was ineffective.

First and most significantly, we found it very difficult to pick a suitable task.  We rejected many tasks because they were too simple.  These tasks did not frustrate users and appeared unlikely benefit from any form of documentation.  On the other hand, the tasks we did choose were frustrating, but primarily due to a lack of informal, high level documentation.  The formal properties that javagrok inferred proved to be mostly irrelevant.

None the less, many test subjects thought our annotations would be useful.  This leads us to believe that the principal failing of our experimental design was duration of task more than any other factor.  We expect that a longer in-situ industrial study would be more likely to provide meaningful conclusions. (positive or negative)

When we began this project, we chose to find and use pre-existing analyses in order to save time.  In hindsight, this was a questionable choice.  The analyses were surprisingly difficult to get working and integrated together.  Once the analyses were integrated and run, we found that the results were somewhat ill-suited to our documentation purposes. (e.g. the lack of @Nullable annotations noted by one of our test subjects) We now believe that creating special purpose analyses is a more promising and effective approach to automatically annotating documentation.

